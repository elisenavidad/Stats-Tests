{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data Preprocessing.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"ZGs-yEJ1j-Kp","colab_type":"text"},"cell_type":"markdown","source":["# Download Missing Modules\n","If specific modules are not already installed in the notebook, the code will be unable to run effectively.  Modules such as ‘lxml’, and ‘suds’ are not natively installed in the Jupyter kernel.  To manually install these modules, the following code must be run. If the modules have already been downloaded, the system will notify the user, but will not result in an error.  Once this cell has been run, the rest of the code blocks will recognize these modules, and be able to successfully complete the workflow. "]},{"metadata":{"id":"ct82QLksj7VV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install lxml\n","!pip install suds-jurko"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B-TCrcN2BAQY","colab_type":"text"},"cell_type":"markdown","source":["# Access SPT Data\n","Another way to access the SPT data is through the API. The following script can be used to access the MHS data for a single station, or for a list of stations.  \n","\n","To use this script, the user must specify the following variables:\n","1. watershed = the name of the watershed within the SPT\n","2. subbasin = the name of the subbasin within the SPT\n","3. spt_id = the ID number of the streamreach where the station is located\n","4. tethys_token = this token is available through the settings of the Tethys portal\n","5. file_location = file location to save the MHS data as a .csv file.  This file location should end in a file separator \"\\\\\". "]},{"metadata":{"id":"ugFjDirABAQZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import requests\n","from io import StringIO\n","import pandas as pd\n","\n","\n","#Define watershed parameters\n","watershed='South America'\n","subbasin='Continental'\n","spt_id=[177442]\n","tethys_token='6cf48ff8aa834c2b923ba84137d0f34fdbd845a2'\n","file_location='D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\\\'\n","\n","for i in spt_id:\n","    request_params=dict(watershed_name=watershed, subbasin_name=subbasin, reach_id=spt_id, return_format='csv')\n","    request_headers = dict(Authorization='Token '+tethys_token)\n","    res = requests.get('http://tethys-staging.byu.edu/apps/streamflow-prediction-tool/api/GetHistoricData/', params=request_params, headers=request_headers)\n","    csv=res.content\n","    csv=csv.decode('utf-8')\n","    csvfile=file_location +str(i)+'.csv'\n","    data=StringIO(csv)\n","    df_data=pd.read_csv(data, sep=',', header=None, names=['predicted streamflow'], index_col=0, infer_datetime_format=True, skiprows=1)\n","    df_data.to_csv(csvfile,sep=',', index_label='Datetime')\n","    print(df_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yie7X5kYzt5H","colab_type":"text"},"cell_type":"markdown","source":["# Access Observed Data from a Hydroserver\n","The following code block can be used to access observed data from a Hydroserver, parse the file from WaterML, and create a .csv file that can then be merged with the SPT data for analysis.  This workflow consists of two functions, get_hydroserver(), and parse_waterml() that allow the user to access, parse, and download observed data from a Hydroserver.  \n","\n","To use this workflow, the user must input:\n","1. url = the url endpoint for the Hydroserver where the observed data is stored\n","2. site_code = the hydroserver-specific site code that corresponds to the streamreach in question\n","3. variable_code = the data-specific code that corresponds to the data (such as discharge) that the user wants to download. \n","4. start_date = the beginning of the time to be downloaded.  Must be in the format 'YYYY-MM-DD'\n","5. end_date = the beginning of the timeframe to be downloaded. Must in the format 'YYYY-MM-DD'\n","6. csv_file = the file location where the downloaded data will be saved as a .csv file\n","\n"]},{"metadata":{"id":"NhMPBOl-0ZsD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from lxml import etree as ET\n","from suds.client import Client\n","import pandas as pd\n","\n","\n","def get_hydroserver(url, site_code, variable_code, start_date, end_date, auth_token):\n","    try:\n","        client = Client(url)\n","    except:\n","        print('could not connect')\n","    response = client.service.GetValues(site_code,\n","                                        variable_code,\n","                                        start_date,\n","                                        end_date,\n","                                        auth_token)\n","    return response\n","\n","\n","def parse_waterml(waterml_string):\n","    root = ET.fromstring(waterml_string)\n","    x = None\n","    y = None\n","    print('parsing waterml data')\n","    time_series = root.findall(\n","        './/{http://www.cuahsi.org/waterML/1.1/}timeSeries')\n","    nodata = root.findtext(\n","        './/{http://www.cuahsi.org/waterML/1.1/}noDataValue')\n","    variable = root.findtext(\n","        './/{http://www.cuahsi.org/waterML/1.1/}variableName')\n","    for series in time_series:\n","        x = []\n","        y = []\n","        values = series.findall(\n","            './/{http://www.cuahsi.org/waterML/1.1/}value')\n","\n","        for element in values:\n","                date = element.attrib['dateTime']\n","                x.append(date)\n","                v = element.text\n","                if nodata in v or v in nodata:\n","                    value = None\n","                    y.append(value)\n","                else:\n","                    v = float(v)\n","                    y.append(float(v))\n","\n","        if variable is None:\n","            variable = ''\n","        if y == []:\n","            variable = 'no data'\n","    waterml_data = {\n","        'dates': x,\n","        'values': y,\n","    }\n","\n","    return waterml_data\n","\n","\n","#Declare variables\n","url = 'http://brasilia.essi-lab.eu/hsl-br/index.php/default/services/cuahsi_1_1.asmx?WSDL'\n","site_code = 'hsl-br:60781000'\n","variable_code = 'hsl-br:Discharge'\n","start_date = '2014-03-01'\n","end_date = '2014-03-05'\n","csv_file=r'D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\Observed Data\\BRAZIL_DATA.csv'\n","\n","# Most servers don't need an auth_token\n","hydro_string = get_hydroserver(url, site_code, variable_code, start_date, end_date, auth_token=None)\n","hydro_values = parse_waterml(hydro_string)\n","\n","#Write to .csv file\n","df=pd.DataFrame.from_dict(hydro_values)\n","df.to_csv(csv_file, index=False)\n","print(csv_file)\n","print(df)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qeXTsC1_BAQc","colab_type":"text"},"cell_type":"markdown","source":["# Merge Data Workflow\n","This workflow merges two time series, an observed and a predicted series, into one .csv file, which can then be used as an input for the Correlation Analysis workflow, and the Lag Analysis workflow. The workflow consists of a function merge_data(), and then the application of that function to produce the merged data file. \n","\n","To use this workflow, the user must specify:\n","1. recorded_dir = directory where the .csv files with the observed flow data are saved\n","2. interim_dir = directory where the .csv files from the SPT are saved\n","3. merged_dir = directory where the merged .csv files will be saved.  This directory should be the up one level from where the observed and SPT data are saved.  Make sure that this file path ends with a file separator \"\\\\\". \n","4. locations = list of stations to be merged\n"]},{"metadata":{"id":"0so3Gf4vb6TZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"code"},"cell_type":"code","source":["import pandas as pd\n","from os import listdir\n","import glob\n","\n","def merge_data(recorded_data,interim_data,location):\n","    #Importing data into a dataframe\n","    df_recorded = pd.read_csv(recorded_data, delimiter=\",\", header=None, names=['recorded streamflow'], index_col=0, infer_datetime_format=True, skiprows=1)\n","    df_predicted = pd.read_csv(interim_data, delimiter=\",\", header=None, names=['predicted streamflow'], index_col=0, infer_datetime_format=True, skiprows=1)\n","    #Converting the index to datetime type\n","    df_recorded.index = pd.to_datetime(df_recorded.index, infer_datetime_format=True)\n","    df_predicted.index = pd.to_datetime(df_predicted.index, infer_datetime_format=True)\n","    #Joining the two dataframes\n","    df_merged = pd.DataFrame.join(df_predicted, df_recorded).dropna()\n","    df_merged.to_csv(merged_dir+location + \"_merged.csv\",sep=\",\",index_label=\"Datetime\")\n","\n","# Specify variables\n","recorded_dir=r\"D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\Observed Data\"\n","interim_dir=r'D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\SPT Data'\n","merged_dir=r'D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\\\'\n","locations = ['Chazuta','Requena', 'San Regis']\n","\n","recorded_list = listdir(recorded_dir)\n","interim_list = listdir(interim_dir)\n","\n","print(recorded_list)\n","print(interim_list)\n","print(locations)\n","\n","for i,j,k in zip(recorded_list,interim_list,locations):\n","    print(i)\n","    i=recorded_dir+\"\\\\\"+str(i)\n","    j=interim_dir+'\\\\'+str(j)\n","    merge_data(i,j,k)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6XVe8TiyZqZK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}