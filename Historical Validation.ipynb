{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Correlation Analysis Workflow\n",
    "This notebook contains the workflow for validating historical time series data from observed data points and modelled historical streamflow (MHS) data.  The MHS data can be accessed through the Streamflow Prediction Tool (SPT) on the Tethys platfrom. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import hydrostats as hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "The following are the functions defined within the workflow to help organize and run the different metrics\n",
    "\n",
    "### plot_station()\n",
    "The plot_station() generates a plot for each station and saves it as a .png file within the Plots folder, which is also created as part of the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_station(dataframe, stations, folderpath):\n",
    "    # Create folder for plot files to live\n",
    "    plt_filename = folderpath + \"//Plots//\" + str(stations) + \"_plot.png\"\n",
    "    if not os.path.exists(folderpath + \"//Plots\"):\n",
    "        os.makedirs(folderpath + \"//Plots//\")\n",
    "    comp_plot = dataframe[['Observed Flow', 'Predicted Flow']].plot(legend=True, color=['blue', 'green'])\n",
    "    comp_plot.set_title('Observed and Predicted Flow for ' + stations)\n",
    "    comp_plot.set_ylabel('Flow (cms)')\n",
    "    plt.savefig(plt_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation_stats()\n",
    "The correlation_stats function takes a dataframe (df) of merged data, an adjustment which accounts for any negatives, and a beginning and ending timestamp, then calculates all of the metrics on the dataframe. \n",
    "This function calculates the metrics on both a monthly and yearly basis.  The monthly statistic is the result of all the datapoints within a specific month, while the yearly statistic is the result of the entire dataset. The correlation_stats() function calculates the correlation coefficient, mean difference, mean variance, as well as the average observed and predicted flows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlation_stats(df, adjustment, beg, end):\n",
    "    # create Dataframe columns\n",
    "    cor_coeff = {}\n",
    "    mean_diff = {}\n",
    "    mean_var = {}\n",
    "    month_predicted = {}\n",
    "    month_observed = {}\n",
    "    # Monthly metrics\n",
    "    for i in range(beg, end):\n",
    "        log_month_predicted = df.where(df.date.dt.month == i).dropna()['log Predicted']\n",
    "        log_month_observed = df.where(df.date.dt.month == i).dropna()['log Recorded']\n",
    "        cor_coeff[i] = hs.acc(log_month_observed, log_month_predicted)\n",
    "        mean_diff[i] = np.exp(scipy.stats.gmean(log_month_observed) - scipy.stats.gmean(log_month_predicted))\n",
    "        mean_var[i] = np.var(log_month_observed - log_month_predicted)\n",
    "        month_predicted[i] = np.exp(np.mean(log_month_predicted - adjustment))\n",
    "        month_observed[i] = np.exp(np.mean(log_month_observed - adjustment))\n",
    "    # Yearly Metrics\n",
    "    log_pred = df.dropna()['log Predicted']\n",
    "    log_observed = df.dropna()['log Recorded']\n",
    "    cor_coeff[14] = hs.acc(log_observed, log_pred)\n",
    "    mean_diff[14] = np.exp(scipy.stats.gmean(log_observed) - scipy.stats.gmean(log_pred))\n",
    "    mean_var[14] = np.var(log_observed - log_pred)\n",
    "    month_predicted[14] = np.exp(np.mean(log_pred - adjustment))\n",
    "    month_observed[14] = np.exp(np.mean(log_observed - adjustment))\n",
    "\n",
    "    # Define dataframe columns\n",
    "    cor_coeff_df = pd.DataFrame.from_dict(cor_coeff, orient='Index')\n",
    "    mean_df = pd.DataFrame.from_dict(mean_diff, orient='Index')\n",
    "    variance_df = pd.DataFrame.from_dict(mean_var, orient='Index')\n",
    "    predicted_df = pd.DataFrame.from_dict(month_predicted, orient='Index')\n",
    "    observed_df = pd.DataFrame.from_dict(month_observed, orient='Index')\n",
    "    correlation_df = pd.concat([cor_coeff_df, mean_df, observed_df, predicted_df, variance_df], axis=1)\n",
    "    # print(correlation_df)\n",
    "    correlation_df.columns = ['Correlation', 'Mean Difference', 'Observed Flow', 'Predicted Flow',\n",
    "                              'Mean Variance']\n",
    "    return correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error_stats()\n",
    "The error_stats is very similar to the correlation_stats() function, except it calculates the RMSE, RMSLE, NSE, R^2, and spectral angle coefficients for the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_stats(df, beg, end):\n",
    "    rmse = {}\n",
    "    rmse_log = {}\n",
    "    NS_eff = {}\n",
    "    R2 = {}\n",
    "    sa = {}\n",
    "    # calculate metrics by month\n",
    "    for i in range(beg, end):\n",
    "        predicted = df.where(df.date.dt.month == i).dropna()['predicted streamflow']\n",
    "        observed = df.where(df.date.dt.month == i).dropna()['recorded streamflow']\n",
    "        log_month_predicted = df.where(df.date.dt.month == i).dropna()['log Predicted']\n",
    "        log_month_observed = df.where(df.date.dt.month == i).dropna()['log Recorded']\n",
    "        n = predicted.count()\n",
    "        # print n\n",
    "        rmse[i] = hs.rmse(predicted, observed)\n",
    "        rmse_log[i] = hs.rmsle(log_month_predicted, log_month_observed)\n",
    "        NS_eff[i] = hs.E(log_month_predicted, log_month_observed)\n",
    "        R2[i] = hs.r_squared(log_month_predicted, log_month_observed)\n",
    "        sa[i] = hs.sa(predicted, observed)\n",
    "    # Calculate metrics for year\n",
    "    predicted = df.dropna()['predicted streamflow']\n",
    "    observed = df.dropna()['recorded streamflow']\n",
    "    log_predicted = df.dropna()['log Predicted']\n",
    "    log_observed = df.dropna()['log Recorded']\n",
    "    n = predicted.count()\n",
    "    rmse[14] = hs.rmse(predicted, observed)\n",
    "    rmse_log[14] = hs.rmsle(log_predicted, log_observed)\n",
    "    NS_eff[14] = hs.E(log_predicted, log_observed)\n",
    "    R2[14] = hs.r_squared(predicted, observed)\n",
    "    sa[14] = hs.sa(predicted, observed)\n",
    "\n",
    "    rmse_df = pd.DataFrame.from_dict(rmse, orient='Index')\n",
    "    rmse_log_df = pd.DataFrame.from_dict(rmse_log, orient='Index')\n",
    "    NS_eff_df = pd.DataFrame.from_dict(NS_eff, orient='Index')\n",
    "    R2_df = pd.DataFrame.from_dict(R2, orient='Index')\n",
    "    sa_df = pd.DataFrame.from_dict(sa, orient='Index')\n",
    "    error_df = pd.concat([rmse_df, rmse_log_df, NS_eff_df, R2_df, sa_df], axis=1)\n",
    "\n",
    "    error_df.columns = ['RMSE', 'Log RMSE', 'Nash-Sutcliffe Efficiency', 'R^2 Coefficient', 'Spectral Angle']\n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monthly_stats_analysis()\n",
    "The monthly_stats_analysis() function takes the list of stations, the folderpath to the data folder, and the datapath to the actual file, accounts for any negative or zero values within the merged dataset, and runs the correlation_stats() and error_stats() functions.  The resulting dataframe from these two functions are then merged to form a final dataframe, which is printed to a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def monthly_stats_analysis(stations, folderpath, datapath):\n",
    "    df = pd.read_csv(datapath)\n",
    "    df['recorded streamflow'] = df['recorded streamflow'].astype(np.float64)\n",
    "\n",
    "    # Replace 0's with .001\n",
    "    df = df.replace(to_replace=0, value=0.001)\n",
    "    df = df.replace(to_replace=1, value=1.0001)\n",
    "\n",
    "    # Log Transform Data\n",
    "    df['log Predicted'] = np.log(df['predicted streamflow'])\n",
    "    df['log Recorded'] = np.log(df['recorded streamflow'], dtype='float64')\n",
    "\n",
    "    # Account for any negatives:\n",
    "    if min(df['log Predicted']) < 0 or min(df['log Recorded']) < 0:\n",
    "        adjustment = max(abs(min(df['log Predicted'])), abs(min(df['log Recorded']))) + 1\n",
    "        print(adjustment)\n",
    "        df['log Predicted'] = df['log Predicted'] + adjustment\n",
    "        df['log Recorded'] = df['log Recorded'] + adjustment\n",
    "    else:\n",
    "        adjustment = 0\n",
    "\n",
    "    # Sort by Months\n",
    "    df['date'] = pd.to_datetime(df['Datetime'])\n",
    "    df.drop('Datetime', axis=1, inplace=True)\n",
    "    df['difference'] = df[['log Recorded']].sub(df['log Predicted'], axis=0)\n",
    "\n",
    "    # Correlation Statistics\n",
    "    # Months are from 1 to 12, 13 is to calculate yearly statistics\n",
    "    correlation_df = correlation_stats(df, adjustment, 1, 13)\n",
    "    # Error Metrics\n",
    "    error_df = error_stats(df, 1, 13)\n",
    "\n",
    "    # Combine into one dataframe\n",
    "    results_df = pd.concat([correlation_df, error_df], axis=1)\n",
    "    results_df.index.name = \"Month\"\n",
    "    results_df['Station'] = stations\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot Results\n",
    "    plot_station(results_df, stations, folderpath)\n",
    "\n",
    "    # Average values\n",
    "    av_cor = results_df['Correlation'].mean()\n",
    "    av_mean = results_df['Mean Difference'].mean()\n",
    "    av_var = results_df['Mean Variance'].mean()\n",
    "    pred_value = results_df['Predicted Flow']\n",
    "    obs_value = results_df['Observed Flow']\n",
    "    rmse_val = hs.rmse(pred_value, obs_value)\n",
    "    print(av_cor, av_mean, av_var, rmse_val)\n",
    "\n",
    "    # Print Results\n",
    "    datapath_results = folderpath + \"//Results//\" + stations + \"_results.csv\"\n",
    "    if not os.path.exists(folderpath + \"//Results\"):\n",
    "        os.makedirs(folderpath + \"//Results\")\n",
    "    results_df.to_csv(datapath_results, sep=',', index_label=\"Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine_csvs()\n",
    "The combine_csvs() function takes the individual results file from each station and merges them into a single summary file.  This file is saved in the same folder as the individual results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_csvs(folderpath):\n",
    "    # Creates summary csv files for each folder of merged datafiles\n",
    "    results = glob.glob(folderpath + \"*_results.csv\")\n",
    "    df_list = []\n",
    "    for file in tqdm(sorted(results)):\n",
    "        df_list.append(pd.read_csv(file))\n",
    "    df_results = pd.concat(df_list)\n",
    "    df_results.to_csv(folderpath + 'National Results.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "### Set variable names\n",
    "Set the variable foldername to be the name of the folder where the merged data files are found.  The folderpath sets the directory where the rest of the analysis will be run. \n",
    "The workflow then makes a list of the stations found, and loops through that list to run the metrics for each station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername=\"\\\\Peru\\\\\"\n",
    "folderpath=\"D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\" + foldername\n",
    "# Create list of stations in folder\n",
    "stations = [os.path.basename(x) for x in glob.glob(folderpath + \"*_merged.csv\")]\n",
    "stations = [s.replace('_merged.csv', '') for s in stations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through stations\n",
    "For each station in the stations list, the workflow runs the monthly_stats_analysis function to calculate the metrics for each station, create a plot of the monthly flow values, and also creates a final summary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in stations:\n",
    "    station = str(i)\n",
    "    print(station)\n",
    "    datapath = str(folderpath) + station + \"_merged.csv\"\n",
    "    monthly_stats_analysis(station, folderpath, datapath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
