{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Historical Validation.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"djQvrBQ3BAQg","colab_type":"text"},"cell_type":"markdown","source":["# Correlation Analysis Workflow\n","This notebook contains the workflow for validating historical time series data from observed data points and modelled historical streamflow (MHS) data.  The MHS data can be accessed through the Streamflow Prediction Tool (SPT) on the Tethys platfrom. To successfully run the workflow, the following modules must be installed."]},{"metadata":{"id":"UT0BjpTRBAQh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#import necessary modules\n","import pandas as pd\n","import numpy as np\n","import scipy\n","from scipy import stats\n","from tqdm import tqdm\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","import hydrostats as hs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2ICO6BSvBAQk","colab_type":"text"},"cell_type":"markdown","source":["## Functions\n","The following are the functions defined within the workflow to help organize and run the different metrics\n","\n","### plot_station()\n","The plot_station() generates a plot for each station and saves it as a .png file within the Plots folder, which is also created as part of the function. "]},{"metadata":{"id":"C6jou1oDBAQl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def plot_station(dataframe, stations, folderpath):\n","    # Create folder for plot files to live\n","    plt_filename = folderpath + \"//Plots//\" + str(stations) + \"_plot.png\"\n","    if not os.path.exists(folderpath + \"//Plots\"):\n","        os.makedirs(folderpath + \"//Plots//\")\n","    comp_plot = dataframe[['Observed Flow', 'Predicted Flow']].plot(legend=True, color=['blue', 'green'])\n","    comp_plot.set_title('Observed and Predicted Flow for ' + stations)\n","    comp_plot.set_ylabel('Flow (cms)')\n","    plt.savefig(plt_filename)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RkTLdustBAQn","colab_type":"text"},"cell_type":"markdown","source":["### correlation_stats()\n","The correlation_stats function takes a dataframe (df) of merged data, an adjustment which accounts for any negatives, and a beginning and ending timestamp, then calculates all of the metrics on the dataframe. \n","This function calculates the metrics on both a monthly and yearly basis.  The monthly statistic is the result of all the datapoints within a specific month, while the yearly statistic is the result of the entire dataset. The correlation_stats() function calculates the correlation coefficient, mean difference, mean variance, as well as the average observed and predicted flows. "]},{"metadata":{"id":"9oCLpV8UBAQo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def correlation_stats(df, adjustment, beg, end):\n","    # create Dataframe columns\n","    cor_coeff = {}\n","    mean_diff = {}\n","    mean_var = {}\n","    month_predicted = {}\n","    month_observed = {}\n","    # Monthly metrics\n","    for i in range(beg, end):\n","        log_month_predicted = df.where(df.date.dt.month == i).dropna()['log Predicted']\n","        log_month_observed = df.where(df.date.dt.month == i).dropna()['log Recorded']\n","        cor_coeff[i] = hs.acc(log_month_observed, log_month_predicted)\n","        mean_diff[i] = np.exp(scipy.stats.gmean(log_month_observed) - scipy.stats.gmean(log_month_predicted))\n","        mean_var[i] = np.var(log_month_observed - log_month_predicted)\n","        month_predicted[i] = np.exp(np.mean(log_month_predicted - adjustment))\n","        month_observed[i] = np.exp(np.mean(log_month_observed - adjustment))\n","    # Yearly Metrics\n","    log_pred = df.dropna()['log Predicted']\n","    log_observed = df.dropna()['log Recorded']\n","    cor_coeff[14] = hs.acc(log_observed, log_pred)\n","    mean_diff[14] = np.exp(scipy.stats.gmean(log_observed) - scipy.stats.gmean(log_pred))\n","    mean_var[14] = np.var(log_observed - log_pred)\n","    month_predicted[14] = np.exp(np.mean(log_pred - adjustment))\n","    month_observed[14] = np.exp(np.mean(log_observed - adjustment))\n","\n","    # Define dataframe columns\n","    cor_coeff_df = pd.DataFrame.from_dict(cor_coeff, orient='Index')\n","    mean_df = pd.DataFrame.from_dict(mean_diff, orient='Index')\n","    variance_df = pd.DataFrame.from_dict(mean_var, orient='Index')\n","    predicted_df = pd.DataFrame.from_dict(month_predicted, orient='Index')\n","    observed_df = pd.DataFrame.from_dict(month_observed, orient='Index')\n","    correlation_df = pd.concat([cor_coeff_df, mean_df, observed_df, predicted_df, variance_df], axis=1)\n","    # print(correlation_df)\n","    correlation_df.columns = ['Correlation', 'Mean Difference', 'Observed Flow', 'Predicted Flow',\n","                              'Mean Variance']\n","    return correlation_df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6PcuYTu7BAQq","colab_type":"text"},"cell_type":"markdown","source":["### error_stats()\n","The error_stats is very similar to the correlation_stats() function, except it calculates the RMSE, RMSLE, NSE, R^2, and spectral angle coefficients for the dataframe. "]},{"metadata":{"id":"w2_3TYWhBAQq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def error_stats(df, beg, end):\n","    rmse = {}\n","    rmse_log = {}\n","    NS_eff = {}\n","    R2 = {}\n","    sa = {}\n","    # calculate metrics by month\n","    for i in range(beg, end):\n","        predicted = df.where(df.date.dt.month == i).dropna()['predicted streamflow']\n","        observed = df.where(df.date.dt.month == i).dropna()['recorded streamflow']\n","        log_month_predicted = df.where(df.date.dt.month == i).dropna()['log Predicted']\n","        log_month_observed = df.where(df.date.dt.month == i).dropna()['log Recorded']\n","        n = predicted.count()\n","        # print n\n","        rmse[i] = hs.rmse(predicted, observed)\n","        rmse_log[i] = hs.rmsle(log_month_predicted, log_month_observed)\n","        NS_eff[i] = hs.E(log_month_predicted, log_month_observed)\n","        R2[i] = hs.r_squared(log_month_predicted, log_month_observed)\n","        sa[i] = hs.sa(predicted, observed)\n","    # Calculate metrics for year\n","    predicted = df.dropna()['predicted streamflow']\n","    observed = df.dropna()['recorded streamflow']\n","    log_predicted = df.dropna()['log Predicted']\n","    log_observed = df.dropna()['log Recorded']\n","    n = predicted.count()\n","    rmse[14] = hs.rmse(predicted, observed)\n","    rmse_log[14] = hs.rmsle(log_predicted, log_observed)\n","    NS_eff[14] = hs.E(log_predicted, log_observed)\n","    R2[14] = hs.r_squared(predicted, observed)\n","    sa[14] = hs.sa(predicted, observed)\n","\n","    rmse_df = pd.DataFrame.from_dict(rmse, orient='Index')\n","    rmse_log_df = pd.DataFrame.from_dict(rmse_log, orient='Index')\n","    NS_eff_df = pd.DataFrame.from_dict(NS_eff, orient='Index')\n","    R2_df = pd.DataFrame.from_dict(R2, orient='Index')\n","    sa_df = pd.DataFrame.from_dict(sa, orient='Index')\n","    error_df = pd.concat([rmse_df, rmse_log_df, NS_eff_df, R2_df, sa_df], axis=1)\n","\n","    error_df.columns = ['RMSE', 'Log RMSE', 'Nash-Sutcliffe Efficiency', 'R^2 Coefficient', 'Spectral Angle']\n","    return error_df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kTyCSR2MBAQu","colab_type":"text"},"cell_type":"markdown","source":["### monthly_stats_analysis()\n","The monthly_stats_analysis() function takes the list of stations, the folderpath to the data folder, and the datapath to the actual file, accounts for any negative or zero values within the merged dataset, and runs the correlation_stats() and error_stats() functions.  The resulting dataframe from these two functions are then merged to form a final dataframe, which is printed to a .csv file. "]},{"metadata":{"id":"FpaslpH9BAQv","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def monthly_stats_analysis(stations, folderpath, datapath):\n","    df = pd.read_csv(datapath)\n","    df['recorded streamflow'] = df['recorded streamflow'].astype(np.float64)\n","\n","    # Replace 0's with .001\n","    df = df.replace(to_replace=0, value=0.001)\n","    df = df.replace(to_replace=1, value=1.0001)\n","\n","    # Log Transform Data\n","    df['log Predicted'] = np.log(df['predicted streamflow'])\n","    df['log Recorded'] = np.log(df['recorded streamflow'], dtype='float64')\n","\n","    # Account for any negatives:\n","    if min(df['log Predicted']) < 0 or min(df['log Recorded']) < 0:\n","        adjustment = max(abs(min(df['log Predicted'])), abs(min(df['log Recorded']))) + 1\n","        print(adjustment)\n","        df['log Predicted'] = df['log Predicted'] + adjustment\n","        df['log Recorded'] = df['log Recorded'] + adjustment\n","    else:\n","        adjustment = 0\n","\n","    # Sort by Months\n","    df['date'] = pd.to_datetime(df['Datetime'])\n","    df.drop('Datetime', axis=1, inplace=True)\n","    df['difference'] = df[['log Recorded']].sub(df['log Predicted'], axis=0)\n","\n","    # Correlation Statistics\n","    # Months are from 1 to 12, 13 is to calculate yearly statistics\n","    correlation_df = correlation_stats(df, adjustment, 1, 13)\n","    # Error Metrics\n","    error_df = error_stats(df, 1, 13)\n","\n","    # Combine into one dataframe\n","    results_df = pd.concat([correlation_df, error_df], axis=1)\n","    results_df.index.name = \"Month\"\n","    results_df['Station'] = stations\n","    print(results_df)\n","\n","    # Plot Results\n","    plot_station(results_df, stations, folderpath)\n","\n","    # Average values\n","    av_cor = results_df['Correlation'].mean()\n","    av_mean = results_df['Mean Difference'].mean()\n","    av_var = results_df['Mean Variance'].mean()\n","    pred_value = results_df['Predicted Flow']\n","    obs_value = results_df['Observed Flow']\n","    rmse_val = hs.rmse(pred_value, obs_value)\n","    print(av_cor, av_mean, av_var, rmse_val)\n","\n","    # Print Results\n","    datapath_results = folderpath + \"//Results//\" + stations + \"_results.csv\"\n","    if not os.path.exists(folderpath + \"//Results\"):\n","        os.makedirs(folderpath + \"//Results\")\n","    results_df.to_csv(datapath_results, sep=',', index_label=\"Month\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TxiWXeqgBAQz","colab_type":"text"},"cell_type":"markdown","source":["### combine_csvs()\n","The combine_csvs() function takes the individual results file from each station and merges them into a single summary file.  This file is saved in the same folder as the individual results. "]},{"metadata":{"id":"-yPvHp6XBAQ0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def combine_csvs(folderpath):\n","    # Creates summary csv files for each folder of merged datafiles\n","    results = glob.glob(folderpath + \"*_results.csv\")\n","    df_list = []\n","    for file in tqdm(sorted(results)):\n","        df_list.append(pd.read_csv(file))\n","    df_results = pd.concat(df_list)\n","    df_results.to_csv(folderpath + 'National Results.csv', sep=',', index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x_OM0ONPBAQ3","colab_type":"text"},"cell_type":"markdown","source":["## Running the Workflow\n","### Set variable names\n","Set the variable foldername to be the name of the folder where the merged data files are found.  The folderpath sets the directory where the rest of the analysis will be run. \n","The workflow then makes a list of the stations found, and loops through that list to run the metrics for each station. "]},{"metadata":{"id":"Hg74Uv0aBAQ4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["foldername=\"\\\\Peru\\\\\"\n","folderpath=\"D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\" + foldername\n","# Create list of stations in folder\n","stations = [os.path.basename(x) for x in glob.glob(folderpath + \"*_merged.csv\")]\n","stations = [s.replace('_merged.csv', '') for s in stations]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xw-5DesrBAQ6","colab_type":"text"},"cell_type":"markdown","source":["### Loop through stations\n","For each station in the stations list, the workflow runs the monthly_stats_analysis function to calculate the metrics for each station, create a plot of the monthly flow values, and also creates a final summary file."]},{"metadata":{"id":"r6cfIENjBAQ7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["for i in stations:\n","    station = str(i)\n","    print(station)\n","    datapath = str(folderpath) + station + \"_merged.csv\"\n","    monthly_stats_analysis(station, folderpath, datapath)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZBWi3gC6x1_v","colab_type":"text"},"cell_type":"markdown","source":["## Running the Workflow\n","To use the above functions together with the named variables and the loop function, select the code block above with the loop function, and use the command \"ctrl+F8\" or \"Run Before\" command.  This tells the notebook to run all of the code blocks above the cursor.  This initializes all the previously discussed functions, and allows the Python to complete the loop command. "]},{"metadata":{"id":"GKsh1Fu7BARA","colab_type":"text"},"cell_type":"markdown","source":["# Lag Analysis Workflow\n","The lag analysis workflow takes the MHS data, interpolates it using a cubic spline, and then lags it by consecutive time steps to calculate the maximum lag necessary for the optimum spectral angle and R^2 coefficients.  The number of time steps can be specified by the user. The results from the lag analysis are output as .csv files. \n","\n","The user must input:\n","1. file_location = directory where the merged data files are saved. This should be a specific folder of files.\n","2. result_location = directory where the results from the analysis will be saved.\n","3. num_timesteps = number of timesteps for the lag analysis to consider. A timestep is equal to 6 hours."]},{"metadata":{"id":"df9JQrPlBARA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy import interpolate\n","import os\n","from os import listdir\n","\n","#Define file locations\n","file_location=\"D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\Merged Data\\\\\"\n","result_location='D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\Results\\\\'\n","num_timesteps = 15\n","\n","def r_squared(x, y):\n","    return np.corrcoef(x, y)[0, 1] ** 2\n","\n","\n","# importing data as a data-frame\n","def r_squared_time_lag(merged_data_path, location, n=num_timesteps):\n","    # Importing Data to arrays\n","    predicted_streamflow = np.genfromtxt(merged_data_path, dtype='float', delimiter=\",\", skip_header=1, usecols=1)\n","    recorded_streamflow = np.genfromtxt(merged_data_path, dtype='float', delimiter=\",\", skip_header=1, usecols=2)\n","    # Creating new arrays for interpolation\n","    x1 = np.arange(predicted_streamflow.size)\n","    x2 = np.arange(recorded_streamflow.size)\n","    # Cubic spline interpolation for predicted streamflow\n","    tck1 = interpolate.splrep(x1, predicted_streamflow, s=0)\n","    xnew1 = np.arange(0, x1.size-.75, 1 / 4)\n","    predicted_interp = interpolate.splev(xnew1, tck1, der=0)\n","    # cubic spline interpolation for recorded streamflow\n","    tck2 = interpolate.splrep(x2, recorded_streamflow, s=0)\n","    xnew2 = np.arange(0, x2.size-0.75, 1 / 4)\n","    recorded_interp = interpolate.splev(xnew2, tck2, der=0)\n","    # creating an empty array for spectral values\n","    y = np.zeros((n + 1)*2 - 1)\n","    # Looping through the different times with roll function\n","    for lag in range(-n, n + 1):\n","        value = r_squared(np.roll(recorded_interp, lag), predicted_interp)\n","        y[lag + n] = value\n","    return y.max(), y.argmax() - n, location\n","\n","\n","# Creating a list for locations and file paths\n","merged_files = listdir(file_location)\n","print(merged_files)\n","locations = []\n","for i in range(len(merged_files)):\n","    name = merged_files[i][:merged_files[i].find('_')]\n","    locations.append(name)\n","for i in range(len(merged_files)):\n","    merged_files[i] = file_location + merged_files[i]\n","\n","\n","y_max_array = np.zeros(len(locations))\n","y_max_location_array = np.zeros(len(locations))\n","\n","for i, j, k in zip(merged_files, locations, range(len(locations))):\n","    y_max_array[k], y_max_location_array[k], _ = r_squared_time_lag(i, j)\n","\n","value_array = np.column_stack((y_max_array, y_max_location_array))\n","\n","summary_df = pd.DataFrame(data=value_array, index=locations, columns=[\"Max R^2 Value\", \"Time Shift at Max R^2 Value\"])\n","\n","summary_df.to_csv(result_location+'\\\\r_squared_summary_NP.csv', index_label=\"Locations\")\n","\n","\n","def spectral(x, y):\n","    value = np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n","    return value\n","\n","\n","# importing data as a data-frame\n","def spectral_time_lag(merged_data_path, location, n=num_timesteps):\n","    # Importing Data to arrays\n","    predicted_streamflow = np.genfromtxt(merged_data_path, dtype='float', delimiter=\",\", skip_header=1, usecols=1)\n","    recorded_streamflow = np.genfromtxt(merged_data_path, dtype='float', delimiter=\",\", skip_header=1, usecols=2)\n","    # Creating new arrays for interpolation\n","    x1 = np.arange(predicted_streamflow.size)\n","    x2 = np.arange(recorded_streamflow.size)\n","    # Cubic spline interpolation for predicted streamflow\n","    tck1 = interpolate.splrep(x1, predicted_streamflow, s=0)\n","    xnew1 = np.arange(0, x1.size, 1 / 4)\n","    predicted_interp = interpolate.splev(xnew1, tck1, der=0)\n","    # cubic spline interpolation for recorded streamflow\n","    tck2 = interpolate.splrep(x2, recorded_streamflow, s=0)\n","    xnew2 = np.arange(0, x2.size, 1 / 4)\n","    recorded_interp = interpolate.splev(xnew2, tck2, der=0)\n","    # creating an empty array for spectral values\n","    y = np.zeros((n + 1)*2 - 1)\n","    for i in range(-n, n + 1):\n","        value = spectral(np.roll(recorded_interp, i), predicted_interp)\n","        y[i + n] = value\n","    return location, y.max(), y.argmax() - n\n","\n","\n","# Creating a list for locations and file paths\n","merged_files = listdir(file_location)\n","locations = []\n","for i in range(len(merged_files)):\n","    name = merged_files[i][:merged_files[i].find('_')]\n","    locations.append(name)\n","for i in range(len(merged_files)):\n","    merged_files[i] = file_location + merged_files[i]\n","\n","y_max_array = np.zeros(len(locations))\n","y_max_location_array = np.zeros(len(locations))\n","\n","for i, j, k in zip(merged_files, locations, range(len(locations))):\n","    _, y_max_array[k], y_max_location_array[k] = spectral_time_lag(i, j)\n","\n","value_array = np.column_stack((y_max_array, y_max_location_array))\n","\n","summary_df = pd.DataFrame(data=value_array, index=locations, columns=[\"Max Spectral Value\", \"Time Shift at Max \"\n","                                                                                            \"Spectral Value\"])\n","\n","summary_df.to_csv(result_location+'\\\\Spectral_Summary_NP.csv', index_label=\"Locations\")"],"execution_count":0,"outputs":[]}]}