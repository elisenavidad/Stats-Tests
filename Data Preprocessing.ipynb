{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data Preprocessing.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"ZGs-yEJ1j-Kp","colab_type":"text"},"cell_type":"markdown","source":["# Download Missing Modules\n","If specific modules are not already installed in the notebook, the code will be unable to run effectively.  Modules such as ‘lxml’, and ‘suds’ are not natively installed in the Jupyter kernel.  To manually install these modules, the following code must be run. Once this cell has been run, the rest of the code blocks will recognize these modules, and be able to successfully complete the workflow. "]},{"metadata":{"id":"ct82QLksj7VV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install lxml\n","!pip install suds-jurko\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B-TCrcN2BAQY","colab_type":"text"},"cell_type":"markdown","source":["# Access SPT Data\n","Another way to access the SPT data is through the API. The following script can be used to access the MHS data for a single station, or for a list of stations.  \n","\n","To use this script, the user must specify the following variables:\n","1. watershed = the name of the watershed within the SPT\n","2. subbasin = the name of the subbasin within the SPT\n","3. spt_id = the ID number of the streamreach where the station is located\n","4. tethys_token = this token is available through the settings of the Tethys portal\n","5. file_location = file location to save the MHS data as a .csv file"]},{"metadata":{"id":"ugFjDirABAQZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import requests\n","from io import StringIO\n","import pandas as pd\n","\n","\n","#Define watershed parameters\n","watershed='South America'\n","subbasin='Continental'\n","spt_id=[177442]\n","tethys_token='6cf48ff8aa834c2b923ba84137d0f34fdbd845a2'\n","file_location='D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\\\'\n","\n","for i in spt_id:\n","    request_params=dict(watershed_name=watershed, subbasin_name=subbasin, reach_id=spt_id, return_format='csv')\n","    request_headers = dict(Authorization='Token '+tethys_token)\n","    res = requests.get('http://tethys-staging.byu.edu/apps/streamflow-prediction-tool/api/GetHistoricData/', params=request_params, headers=request_headers)\n","    csv=res.content\n","    csv=csv.decode('utf-8')\n","    csvfile=file_location +str(i)+'.csv'\n","    data=StringIO(csv)\n","    df_data=pd.read_csv(data, sep=',', header=None, names=['predicted streamflow'], index_col=0, infer_datetime_format=True, skiprows=1)\n","    df_data.to_csv(csvfile,sep=',', index_label='Datetime')\n","    print(df_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yie7X5kYzt5H","colab_type":"text"},"cell_type":"markdown","source":["# Access Observed Data from a Hydroserver\n","The following code block can be used to access observed data from a Hydroserver, parse the file from WaterML, and create a .csv file that can then be merged with the SPT data for analysis.  This workflow consists of two functions, get_hydroserver(), and parse_waterml() that allow the user to access, parse, and download observed data from a Hydroserver.  \n","\n","To use this workflow, the user must input:\n","1. url = the url endpoint for the Hydroserver where the observed data is stored\n","2. site_code = the hydroserver-specific code that corresponds to the streamreach in question\n","3. variable_code = the data-specific code that corresponds to the data (such as discharge) that the user wants to download. \n","4. start_date = the beginning of the time to be downloaded.  Must be in the format 'YYYY-MM-DD'\n","5. end_date = the beginning of the timeframe to be downloaded. Must in the format 'YYYY-MM-DD'\n","6. csv_file = the file location where the downloaded data will be saved as a .csv file\n","\n"]},{"metadata":{"id":"NhMPBOl-0ZsD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":1139},"outputId":"43ec6b93-66c0-462a-b291-ac59c31e162f","executionInfo":{"status":"ok","timestamp":1522696514007,"user_tz":360,"elapsed":927,"user":{"displayName":"Elise Jackson","photoUrl":"//lh3.googleusercontent.com/-HitMOHMfifo/AAAAAAAAAAI/AAAAAAAAD50/NopkGZdl3-0/s50-c-k-no/photo.jpg","userId":"108852378785739184638"}}},"cell_type":"code","source":["from lxml import etree as ET\n","from suds.client import Client\n","import pandas as pd\n","\n","\n","def get_hydroserver(url, site_code, variable_code, start_date, end_date, auth_token):\n","    try:\n","        client = Client(url)\n","    except:\n","        print('could not connect')\n","    response = client.service.GetValues(site_code,\n","                                        variable_code,\n","                                        start_date,\n","                                        end_date,\n","                                        auth_token)\n","    return response\n","\n","\n","def parse_waterml(waterml_string):\n","    root = ET.fromstring(waterml_string)\n","    x = None\n","    y = None\n","    print('parsing waterml data')\n","    time_series = root.findall(\n","        './/{http://www.cuahsi.org/waterML/1.1/}timeSeries')\n","    nodata = root.findtext(\n","        './/{http://www.cuahsi.org/waterML/1.1/}noDataValue')\n","    variable = root.findtext(\n","        './/{http://www.cuahsi.org/waterML/1.1/}variableName')\n","    for series in time_series:\n","        x = []\n","        y = []\n","        values = series.findall(\n","            './/{http://www.cuahsi.org/waterML/1.1/}value')\n","\n","        for element in values:\n","                date = element.attrib['dateTime']\n","                x.append(date)\n","                v = element.text\n","                if nodata in v or v in nodata:\n","                    value = None\n","                    y.append(value)\n","                else:\n","                    v = float(v)\n","                    y.append(float(v))\n","\n","        if variable is None:\n","            variable = ''\n","        if y == []:\n","            variable = 'no data'\n","    waterml_data = {\n","        'dates': x,\n","        'values': y,\n","    }\n","\n","    return waterml_data\n","\n","\n","#Declare variables\n","url = 'http://brasilia.essi-lab.eu/hsl-br/index.php/default/services/cuahsi_1_1.asmx?WSDL'\n","site_code = 'hsl-br:60781000'\n","variable_code = 'hsl-br:Discharge'\n","start_date = '2014-03-01'\n","end_date = '2014-03-05'\n","csv_file=r'D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\Observed Data\\BRAZIL_DATA.csv'\n","\n","# Most servers don't need an auth_token\n","hydro_string = get_hydroserver(url, site_code, variable_code, start_date, end_date, auth_token=None)\n","hydro_values = parse_waterml(hydro_string)\n","\n","#Write to .csv file\n","df=pd.DataFrame.from_dict(hydro_values)\n","df.to_csv(csv_file, index=False)\n","print(csv_file)\n","print(df)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["parsing waterml data\n","D:\\Jackson\\Streamflow Prediction\\Data Analysis\\Python Stats\\Peru\\Observed Data\\BRAZIL_DATA.csv\n","                   dates  values\n","0    2014-03-01T00:00:00   172.0\n","1    2014-03-01T00:15:00   172.0\n","2    2014-03-01T00:30:00   172.0\n","3    2014-03-01T00:45:00   172.0\n","4    2014-03-01T01:00:00   172.0\n","5    2014-03-01T01:15:00   172.0\n","6    2014-03-01T01:30:00   172.0\n","7    2014-03-01T01:45:00   172.0\n","8    2014-03-01T02:00:00   172.0\n","9    2014-03-01T02:15:00   172.0\n","10   2014-03-01T02:30:00   172.0\n","11   2014-03-01T02:45:00   172.0\n","12   2014-03-01T03:00:00   172.0\n","13   2014-03-01T03:15:00   172.0\n","14   2014-03-01T03:30:00   172.0\n","15   2014-03-01T03:45:00   172.0\n","16   2014-03-01T04:00:00   171.0\n","17   2014-03-01T04:15:00   171.0\n","18   2014-03-01T04:30:00   171.0\n","19   2014-03-01T04:45:00   171.0\n","20   2014-03-01T05:00:00   171.0\n","21   2014-03-01T05:15:00   171.0\n","22   2014-03-01T05:30:00   171.0\n","23   2014-03-01T05:45:00   171.0\n","24   2014-03-01T06:00:00   171.0\n","25   2014-03-01T06:15:00   171.0\n","26   2014-03-01T06:30:00   171.0\n","27   2014-03-01T06:45:00   171.0\n","28   2014-03-01T07:00:00   171.0\n","29   2014-03-01T07:15:00   171.0\n","..                   ...     ...\n","347  2014-03-04T16:45:00   184.0\n","348  2014-03-04T17:00:00   184.0\n","349  2014-03-04T17:15:00   184.0\n","350  2014-03-04T17:30:00   184.0\n","351  2014-03-04T17:45:00   183.0\n","352  2014-03-04T18:00:00   183.0\n","353  2014-03-04T18:15:00   182.0\n","354  2014-03-04T18:30:00   182.0\n","355  2014-03-04T18:45:00   182.0\n","356  2014-03-04T19:00:00   182.0\n","357  2014-03-04T19:15:00   181.0\n","358  2014-03-04T19:30:00   181.0\n","359  2014-03-04T19:45:00   181.0\n","360  2014-03-04T20:00:00   181.0\n","361  2014-03-04T20:15:00   181.0\n","362  2014-03-04T20:30:00   180.0\n","363  2014-03-04T20:45:00   180.0\n","364  2014-03-04T21:00:00   180.0\n","365  2014-03-04T21:15:00   180.0\n","366  2014-03-04T21:30:00   180.0\n","367  2014-03-04T21:45:00   180.0\n","368  2014-03-04T22:00:00   180.0\n","369  2014-03-04T22:15:00   180.0\n","370  2014-03-04T22:30:00   179.0\n","371  2014-03-04T22:45:00   179.0\n","372  2014-03-04T23:00:00   179.0\n","373  2014-03-04T23:15:00   179.0\n","374  2014-03-04T23:30:00   179.0\n","375  2014-03-04T23:45:00   179.0\n","376  2014-03-05T00:00:00   179.0\n","\n","[377 rows x 2 columns]\n"],"name":"stdout"}]},{"metadata":{"id":"qeXTsC1_BAQc","colab_type":"text"},"cell_type":"markdown","source":["# Merge Data Workflow\n","This workflow merges two time series, an observed and a predicted series, into one .csv file, which can then be used as an input for the Correlation Analysis workflow, and the Lag Analysis workflow. The workflow consists of a function merge_data(), and then the application of that function to produce the merged data file. \n","\n","To use this workflow, the user must specify:\n","1. recorded_dir = directory where the .csv files with the observed flow data are saved\n","2. interim_dir = directory where the .csv files from the SPT are saved\n","3. merged_dir = directory where the merged .csv files will be saved.  This directory should be the up one level from where the observed and SPT data are saved. \n","4. locations = list of stations to be merged\n"]},{"metadata":{"id":"0so3Gf4vb6TZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"code"},"cell_type":"code","source":["import pandas as pd\n","from os import listdir\n","import glob\n","\n","def merge_data(recorded_data,interim_data,location):\n","    #Importing data into a dataframe\n","    df_recorded = pd.read_csv(recorded_data, delimiter=\",\", header=None, names=['recorded streamflow'], index_col=0, infer_datetime_format=True, skiprows=1)\n","    df_predicted = pd.read_csv(interim_data, delimiter=\",\", header=None, names=['predicted streamflow'], index_col=0, infer_datetime_format=True, skiprows=1)\n","    #Converting the index to datetime type\n","    df_recorded.index = pd.to_datetime(df_recorded.index, infer_datetime_format=True)\n","    df_predicted.index = pd.to_datetime(df_predicted.index, infer_datetime_format=True)\n","    #Joining the two dataframes\n","    df_merged = pd.DataFrame.join(df_predicted, df_recorded).dropna()\n","    df_merged.to_csv(merged_dir+location + \"_merged.csv\",sep=\",\",index_label=\"Datetime\")\n","\n","# Specify variables\n","recorded_dir=r\"C:\\Users\\Owner\\Documents\\School\\Research\\Peru\\Peru\\Observed Data\"\n","interim_dir=r'C:\\Users\\Owner\\Documents\\School\\Research\\Peru\\Peru\\SPT Data'\n","merged_dir=r'C:\\Users\\Owner\\Documents\\School\\Research\\Peru\\Peru\\\\'\n","locations = ['Chazuta','Requena', 'San Regis']\n","\n","recorded_list = listdir(recorded_dir)\n","interim_list = listdir(interim_dir)\n","\n","print(recorded_list)\n","print(interim_list)\n","print(locations)\n","\n","for i,j,k in zip(recorded_list,interim_list,locations):\n","    print(i)\n","    i=recorded_dir+\"\\\\\"+str(i)\n","    j=interim_dir+'\\\\'+str(j)\n","    merge_data(i,j,k)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7JauYt9Ww5OU","colab_type":"text"},"cell_type":"markdown","source":["# Python Modules to Import\n","As part of the computer setup, specific modules must be imported, so that Python can complete the necessary calculations."]},{"metadata":{"id":"UT0BjpTRBAQh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#import necessary modules\n","import pandas as pd\n","import numpy as np\n","import scipy\n","from scipy import stats\n","from tqdm import tqdm\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","import hydrostats as hs"],"execution_count":0,"outputs":[]}]}